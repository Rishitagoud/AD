{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMRwRQ3sJ9AdmUQVXmbXZSN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rishitagoud/AD/blob/main/excelR_Assignments.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NaJg12jyxXsM",
        "outputId": "7f2693b4-4829-43f4-a115-2fa6a43e993b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Integer: 10, Type: <class 'int'>\n",
            "Float: 20.5, Type: <class 'float'>\n",
            "String: Hello, Jupyter!, Type: <class 'str'>\n",
            "Boolean: True, Type: <class 'bool'>\n"
          ]
        }
      ],
      "source": [
        "#day2,1. Jupyter Notebook and Data Types . Write a program in Jupyter Notebook to declare variables of different data types (integer, float, string, and boolean). Print each variable and its type.\n",
        "\n",
        "integer_var = 10\n",
        "float_var = 20.5\n",
        "string_var = \"Hello, Jupyter!\"\n",
        "boolean_var = True\n",
        "print(f\"Integer: {integer_var}, Type: {type(integer_var)}\")\n",
        "print(f\"Float: {float_var}, Type: {type(float_var)}\")\n",
        "print(f\"String: {string_var}, Type: {type(string_var)}\")\n",
        "print(f\"Boolean: {boolean_var}, Type: {type(boolean_var)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#day-3,2Create a List, tuple and Dictionary with 5 elements in it and how to access few elements based on the index. Try  with different examples\n",
        "integer_var = 10\n",
        "float_var = 20.5\n",
        "string_var = \"Hello, Jupyter!\"\n",
        "boolean_var = True\n",
        "print(f\"Integer: {integer_var}, Type: {type(integer_var)}\")\n",
        "print(f\"Float: {float_var}, Type: {type(float_var)}\")\n",
        "print(f\"String: {string_var}, Type: {type(string_var)}\")\n",
        "print(f\"Boolean: {boolean_var}, Type: {type(boolean_var)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t8D-AOK1xnwI",
        "outputId": "50070156-f97e-43ee-e30e-e77661abeb4e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Integer: 10, Type: <class 'int'>\n",
            "Float: 20.5, Type: <class 'float'>\n",
            "String: Hello, Jupyter!, Type: <class 'str'>\n",
            "Boolean: True, Type: <class 'bool'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#day4,3Write a Python program that takes a student's marks in three subjects as input. If the average is greater than or equal to 90, print \"Grade: A\". If the average is between 80 and 89, print \"Grade: B\". If the average is between 70 and 79, print \"Grade: C\". Otherwise, print \"Grade: Fail\". in python\n",
        "# Taking input for marks in three subjects\n",
        "subject1 = float(input(\"Enter marks for Subject 1: \"))\n",
        "subject2 = float(input(\"Enter marks for Subject 2: \"))\n",
        "subject3 = float(input(\"Enter marks for Subject 3: \"))\n",
        "\n",
        "average = (subject1 + subject2 + subject3) / 3\n",
        "\n",
        "if average >= 90:\n",
        "    print(\"Grade: A\")\n",
        "elif 80 <= average < 90:\n",
        "    print(\"Grade: B\")\n",
        "elif 70 <= average < 80:\n",
        "    print(\"Grade: C\")\n",
        "else:\n",
        "    print(\"Grade: Fail\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ppLol4_AxoLn",
        "outputId": "0efe4140-a8ac-45f7-9bcb-f3a89cf2ae8f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter marks for Subject 1: 30\n",
            "Enter marks for Subject 2: 39\n",
            "Enter marks for Subject 3: 40\n",
            "Grade: Fail\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#day-4,3bWrite a Python program to calculate the sum of all even numbers between 1 and a given positive integer n\n",
        "def sum_of_evens(n):\n",
        "    # Using list comprehension and sum function\n",
        "    return sum(i for i in range(2, n + 1, 2))\n",
        "\n",
        "# Taking input from the user\n",
        "n = int(input(\"Enter a positive integer: \"))\n",
        "\n",
        "# Ensuring the input is positive\n",
        "if n <= 0:\n",
        "    print(\"Please enter a positive integer.\")\n",
        "else:\n",
        "    result = sum_of_evens(n)\n",
        "    print(f\"The sum of all even numbers between 1 and {n} is: {result}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VZL2yUQ6xotG",
        "outputId": "a6008dd2-55e3-400e-be4f-0f5173ad6085"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a positive integer: 3\n",
            "The sum of all even numbers between 1 and 3 is: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#day-54write a python program to calculate frequency of each word in given text . print words and their corresponding counts\n",
        "from collections import Counter\n",
        "\n",
        "def word_frequency(text):\n",
        "    # Convert text to lowercase and split into words\n",
        "    words = text.lower().split()\n",
        "\n",
        "    # Count the frequency of each word\n",
        "    word_counts = Counter(words)\n",
        "\n",
        "    # Print words and their corresponding counts\n",
        "    for word, count in word_counts.items():\n",
        "        print(f\"{word}: {count}\")\n",
        "\n",
        "# Example usage\n",
        "text = input(\"Enter a text: \")\n",
        "word_frequency(text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m1rEi6lNyKXM",
        "outputId": "a48cfa16-0bbf-4519-8649-438c7b9063f3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a text: a\n",
            "a: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#day-7,6Write a Python program to using NLTK and Spacy Convert text to lowercase. Remove stopwords using NLTK\n",
        "import nltk\n",
        "import spacy\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "\n",
        "# Download stopwords if not already downloaded\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Load Spacy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def word_frequency(text):\n",
        "    # Convert text to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Process text using Spacy\n",
        "    doc = nlp(text)\n",
        "\n",
        "    # Tokenize and remove stopwords\n",
        "    words = [token.text for token in doc if token.text not in stop_words and token.is_alpha]\n",
        "\n",
        "    # Count the frequency of each word\n",
        "    word_counts = Counter(words)\n",
        "\n",
        "    # Print words and their corresponding counts\n",
        "    for word, count in word_counts.items():\n",
        "        print(f\"{word}: {count}\")\n",
        "\n",
        "# Example usage\n",
        "text = input(\"Enter a text: \")\n",
        "word_frequency(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RHv0WEV0y3IA",
        "outputId": "ca2bb5e5-055a-408d-cb47-62700d127ae5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter a text: t\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#day9\n",
        "import gensim\n",
        "import gensim.downloader as api\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.parsing.preprocessing import remove_stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "import string\n",
        "import os\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove punctuation\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "    # Tokenization\n",
        "    tokens = simple_preprocess(text)\n",
        "\n",
        "    # Remove stopwords\n",
        "    tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
        "\n",
        "    # Stemming\n",
        "    stemmer = PorterStemmer()\n",
        "    stemmed_tokens = [stemmer.stem(word) for word in tokens]\n",
        "\n",
        "    # Lemmatization\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in stemmed_tokens]\n",
        "\n",
        "    return lemmatized_tokens\n",
        "\n",
        "# Check if sample text file exists\n",
        "file_path = \"sample_text.txt\"\n",
        "if os.path.exists(file_path):\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "        text_data = file.read()\n",
        "    # Preprocess text\n",
        "    processed_tokens = preprocess_text(text_data)\n",
        "    print(\"Processed Tokens:\", processed_tokens)\n",
        "else:\n",
        "    print(f\"Error: File '{file_path}' not found. Please provide a valid text file.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EJNn9Ii21QwF",
        "outputId": "30be3f77-b035-4320-a8a8-fab7a1e1ea7b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: File 'sample_text.txt' not found. Please provide a valid text file.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#day11\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "import os\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt')\n",
        "\n",
        "def tokenize_text(text):\n",
        "    # Tokenize into sentences\n",
        "    sentences = sent_tokenize(text)\n",
        "\n",
        "    # Tokenize into words\n",
        "    words = word_tokenize(text)\n",
        "\n",
        "    return sentences, words\n",
        "\n",
        "# Check if sample text file exists\n",
        "file_path = \"sample_text.txt\"\n",
        "if os.path.exists(file_path):\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "        text_data = file.read()\n",
        "\n",
        "    # Tokenize text\n",
        "    sentences, words = tokenize_text(text_data)\n",
        "    print(\"Tokenized Sentences:\", sentences)\n",
        "    print(\"Tokenized Words:\", words)\n",
        "else:\n",
        "    print(f\"Error: File '{file_path}' not found. Please provide a valid text file.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xKWpitg0z8yd",
        "outputId": "9f186626-2778-4da7-8959-6801e650ffcf"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: File 'sample_text.txt' not found. Please provide a valid text file.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#day13\n",
        "import re\n",
        "\n",
        "def clean_text(text):\n",
        "    # Convert text to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove special characters using regex\n",
        "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
        "\n",
        "    return text\n",
        "\n",
        "# Test the function\n",
        "sample_text = 'Hello, World! Welcome to NLP 101.'\n",
        "cleaned_text = clean_text(sample_text)\n",
        "print(\"Cleaned Text:\", cleaned_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QMlwq8Fm0-Gv",
        "outputId": "03ddc7b1-4f91-4dfc-b3f7-713333b43ff1"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned Text: hello world welcome to nlp 101\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#day15\n",
        "import re\n",
        "\n",
        "def extract_emails(text):\n",
        "    # Regular expression pattern for extracting emails\n",
        "    email_pattern = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'\n",
        "\n",
        "    # Find all email addresses in the text\n",
        "    emails = re.findall(email_pattern, text)\n",
        "\n",
        "    return emails\n",
        "\n",
        "# Test the function\n",
        "sample_text = 'Contact us at support@example.com and sales@example.org.'\n",
        "extracted_emails = extract_emails(sample_text)\n",
        "print(\"Extracted Emails:\", extracted_emails)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rKlI1r-z1gXy",
        "outputId": "fa76b8cc-f1af-424c-db7a-b4df32aca269"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted Emails: ['support@example.com', 'sales@example.org']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#day18"
      ],
      "metadata": {
        "id": "3fxbcHJ211RW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}